{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4XCLQ3xR8nKcfA6l2DXxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lijiacheng2001/note-warehouse/blob/master/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8EAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "07jHlpgePjvX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "#nltk.download()\n",
        "\n",
        "import jieba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split('\\t')      # 每一行是英文+翻译的形式\n",
        "            #print(line)   # ['Anyone can do that.', '任何人都可以做到。']\n",
        "            #print(nltk.word_tokenize(line[0].lower()))    # ['anyone', 'can', 'do', 'that', '.']\n",
        "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
        "            #print([c for c in line[1]])   ['任', '何', '人', '都', '可', '以', '做', '到', '。']\n",
        "            #print(list(jieba.cut(line[1])))        ['任何人', '都', '可以', '做到', '。']\n",
        "            #cn.append(['BOS'] + [c for c in line[1]] + ['EOS'])\n",
        "            cn.append(['BOS'] + list(jieba.cut(line[1])) + ['EOS'])\n",
        "    return en, cn\n",
        "\n",
        "train_file = 'nmt/en-cn/train.txt'\n",
        "dev_file = 'nmt/en-cn/dev.txt'\n",
        "train_en, train_cn = load_data(train_file)\n",
        "dev_en, dev_cn = load_data(dev_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "collapsed": true,
        "id": "hyBisKJxRz1x",
        "outputId": "364ffea4-135d-467e-d3d7-919f999b4bd7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'nmt/en-cn/train.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-30675360dc3e>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nmt/en-cn/train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdev_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nmt/en-cn/dev.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdev_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_cn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-30675360dc3e>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(in_file)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# 每一行是英文+翻译的形式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nmt/en-cn/train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "    total_words = len(ls) + 2    # 两个特殊的字符UNK和PAD\n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}   # 字典的前两个位置放特殊字符\n",
        "    word_dict['UNK'] = UNK_IDX\n",
        "    word_dict['PAD'] = PAD_IDX\n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "\n",
        "inv_en_dict = {v:k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v:k for k, v in cn_dict.items()}"
      ],
      "metadata": {
        "id": "IdMolRFER3az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
        "    length = len(en_sentences)\n",
        "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
        "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
        "    # 根据英语句子的长度排序\n",
        "    def len_argsort(seq):   # 这个seq是一个二维矩阵， 每一行是一个句子， 且都已经用单词在字典中的位置进行了编码\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "\n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(out_en_sentences)\n",
        "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
        "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
        "\n",
        "    return out_en_sentences, out_cn_sentences\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "metadata": {
        "id": "7Uf9CYBZSQHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 这个函数的作用是我们输入训练集的样本个数， batch_size大小， 就会返回多批 连续的batch_size个索引， 每一个索引代表一个样本\n",
        "# 也就是可以根据这个索引去拿到一个个的batch\n",
        "def get_minibatches(n, minibatch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, minibatch_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "    minibatches = []\n",
        "    for idx in idx_list:\n",
        "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
        "    return minibatches      # 这个会返回多批连着的bath_size个索引\n",
        "#get_minibatches(len(train_en), 32)\n",
        "\n",
        "# 这个函数是在做数据预处理， 由于每个句子都不是一样长， 所以通过这个函数就可以把句子进行补齐， 不够长的在句子后面添加0\n",
        "def prepare_data(seqs):\n",
        "    lengths = [len(seq) for seq in seqs]    # 得到每个句子的长度\n",
        "    n_samples = len(seqs)       # 得到一共有多少个句子\n",
        "    max_len = np.max(lengths)              # 找出最大的句子长度\n",
        "\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')    # 按照最大句子长度生成全0矩阵\n",
        "    x_lengths = np.array(lengths).astype('int32')\n",
        "    for idx, seq in enumerate(seqs):        # 把有句子的位置填充进去\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "    return x, x_lengths      # x_mask\n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    minibatches = get_minibatches(len(en_sentences), batch_size)   # 得到batch个索引\n",
        "    all_ex = []\n",
        "    for minibatch in minibatches:   # 每批数据的索引\n",
        "        mb_en_sentences = [en_sentences[t] for t in minibatch]   # 取数据\n",
        "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]  # 取数据\n",
        "        mb_x, mb_x_len = prepare_data(mb_en_sentences) # 填充成一样的长度， 但是要记录一下句子的真实长度， 这个在后面输入网络的时候得用\n",
        "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "    return all_ex\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)   # 产生训练集\n",
        "random.shuffle(train_data)\n",
        "dev_data = gen_examples(dev_en, dev_cn, batch_size)   # 产生验证集"
      ],
      "metadata": {
        "id": "9P_LkQ6ZSTDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[1][0].shape, train_data[1][1].shape, train_data[1][2].shape, train_data[1][3].shape)\n",
        "# 第一个维度表示第1个batch， 第二个维度[0]代表每个每个句子单词个数， [1]代表每个句子的长度， [2]代表中文词个数， [3]代表每个句子的中文长度\n",
        "# 注意每个batch里面的句子长度是不一样的， 同一batch里面的句子长度由于填充0使得一样了\n",
        "\n",
        "(64, 11) (64,) (64, 14) (64,)"
      ],
      "metadata": {
        "id": "XJSK9R1FSUum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "简单的Seq2Seq模型"
      ],
      "metadata": {
        "id": "vBwL48Q8TQlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "tensor_in = torch.FloatTensor([[1, 2, 3], [5, 0, 0]]).resize_(2, 3, 1)\n",
        "tensor_in = Variable(tensor_in)\n",
        "seq_lenghs = [3, 1]\n",
        "tensor_in\n",
        "\n",
        "## 结果\n",
        "tensor([[[1.],\n",
        "         [2.],\n",
        "         [3.]],\n",
        "\n",
        "        [[5.],\n",
        "         [0.],\n",
        "         [0.]]])"
      ],
      "metadata": {
        "id": "7u-dCnTASdl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(seq_lenghs).sort(0, descending=True)\n",
        "\n",
        "#  结果：  这句会返回两个值， 第一个是排好序的数组， 第二个是每个元素在原数组里面的位置\n",
        "torch.return_types.sort(\n",
        "values=tensor([3, 1]),\n",
        "indices=tensor([0, 1]))"
      ],
      "metadata": {
        "id": "C2P_vcQTSeU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pack = nn.utils.rnn.pack_padded_sequence(tensor_in, seq_lenghs, batch_first=True)\n",
        "pack\n",
        "\n",
        "## 会发现数据变成了这样：\n",
        "PackedSequence(data=tensor([[1.],\n",
        "        [5.],\n",
        "        [2.],\n",
        "        [3.]]), batch_sizes=tensor([2, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ],
      "metadata": {
        "id": "yEZPljC9SfxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.RNN(1, 2, 3, batch_first=True)   # 输入维度是1(embed_dim)， 输出维度是2(2个隐藏单元), 3层\n",
        "h0 = Variable(torch.randn(3, 2, 2))  # h0的初始状态， (layers_num*direction_nums, batch_size, hidden_size)\n",
        "\n",
        "out, h = rnn(pack, h0)\n",
        "out[0].shape   # [4, 2]\n",
        "out\n",
        "\n",
        "## 结果：   这其实就是上面那四个关键的单词经过RNN之后的2个隐藏状态输出\n",
        "tensor([[-0.8499,  0.0821],                  # 对应1\n",
        "        [-0.7253, -0.4307],                 # 对应5\n",
        "        [-0.8461,  0.6166],                 # 对应2\n",
        "        [-0.8453,  0.4468]], grad_fn=<CatBackward>)  # 对应3"
      ],
      "metadata": {
        "id": "Gp_j0hDfShx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainEncoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        #这里需要输入lengths, 因为每个句子是不一样长的，我们需要每个句子最后一个时间步的隐藏状态，\n",
        "        #所以需要知道句子有多长，x表示一个batch里面的句子\n",
        "\n",
        "        #把batch里面的seq按照长度排序\n",
        "        sorten, sorted_idx = lengths.sort(0, descending=True) #sorten表示排好序的数组，sorted_index表示每个元素再原数组位置\n",
        "        x_sorted = x[sorted_idx.long()] #句子已经按照seq长度排好序\n",
        "        embedded = self.dropout(self.embed(x_sorted)) #[batch_size, seq_len, embed_size]\n",
        "\n",
        "        #下面一段代码处理变长序列\n",
        "        #这里的data.numpy()是原始张量的克隆，然后转成了numpy数组，相当于clone().numpy()\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        #上面这句话之后，会把变长序列的0都给去掉，之前填充的字符都给压扁\n",
        "        packed_out, hid = self.rnn(packed_embedded)#通过这句话就可以得到batch中每个样本的真实隐藏状态\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)#这里是再填充回去，看下面的例子就懂了\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False) #这里是为了还是让短的句子再前面\n",
        "        out = out[original_idx.long()].contiguous() #contiguous是为了把不连续的内存单元连续起来\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        return out, hid[[-1]] #把最后一层的his给拿出来，这个具体看上面的简单演示"
      ],
      "metadata": {
        "id": "--tDLDpJSlhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#这个基本上和Encoder是一致得，无非就是初始化得h换成了Encoder之后得h\n",
        "class PlainDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
        "        super(PlainDecoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, y, y_lengths, hid):\n",
        "        #y:[batch_size, seq_len-1]\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True) #依然是句子从长到短排序\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "\n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) #[batch_size, output_length, embed_size]\n",
        "\n",
        "        pack_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(pack_seq, hid) #这个计算得是每个有效时间步单词得最后一层得隐藏状态\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True) #[batch,seq_len-1, hidden_size]\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguouts() #[batch, seq_len-1, hidden_size]\n",
        "\n",
        "        hid = hid[:, original_idx.long()].contiguous() #[1,batch, hidden_size]\n",
        "        output = F.log_softmax(self.out(output_seq), -1)\n",
        "        #[batch, seq_len-1, vocab_size] 表示每个样本每个时间步长都有一个vocab_size得维度长度，表示每个单词得概率\n",
        "\n",
        "        return output, hid"
      ],
      "metadata": {
        "id": "KUExJuYWSn03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(PlainSeq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths) #encoder进行编码\n",
        "        output, hid = self.decoder(y, y_lengths, hid) #decoder负责解码\n",
        "        return output, None\n",
        "    def translate(self, x, x_lengths, y, max_length=10):#这个是进来一个句子进行翻译 max_length句子得最大长度\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "        return torch.cat(preds, 1), None"
      ],
      "metadata": {
        "id": "_vCCU1d1S0aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# masked cross entropy loss\n",
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        # input: [batch_size, seq_len, vocab_size]    每个单词的可能性\n",
        "        input = input.contiguous().view(-1, input.size(2))   # [batch_size*seq_len-1, vocab_size]\n",
        "        target = target.contiguous().view(-1, 1)    #  [batch_size*seq_len-1, 1]\n",
        "\n",
        "        mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
        "        output = -input.gather(1, target) * mask # 在每个vocab_size维度取正确单词的索引， 但是里面有很多是填充进去的， 所以mask去掉这些填充的\n",
        "        # 这个其实在写一个NLloss ， 也就是sortmax的取负号\n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "\n",
        "        return output  # [batch_size*seq_len-1, 1]"
      ],
      "metadata": {
        "id": "9kEyiGc0S1vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dropout = 0.2\n",
        "hidden_size = 100\n",
        "encoder = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
        "\n",
        "model = PlainSeq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "loss_fn = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "tMbMMlCwS24x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义训练和验证函数\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()    # 这个是一个batch的英文句子 大小是[batch_size, seq_len]\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()    # 每个句子的长度\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()  # 解码器那边的输入， 输入一个单词去预测另外一个单词\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()   # 解码器那边的输出  [batch_size, seq_len-1]\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()  # 这个减去1， 因为没有了最后一个  [batch_size, seq_len-1]\n",
        "            mb_y_len[mb_y_len<=0] =  1   # 这句话是为了以防出错\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            # [batch_size, mb_y_len.max()], 上面是bool类型， 下面是float类型， 只计算每个句子的有效部分， 填充的那部分去掉\n",
        "            mb_out_mask = mb_out_mask.float()  # [batch_size, seq_len-1]  因为mb_y_len.max()就是seq_len-1\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "    print('Evaluation loss', total_loss / total_num_words)\n",
        "\n",
        "def train(model, data, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_num_words = total_loss = 0.\n",
        "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
        "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
        "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
        "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
        "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
        "            mb_y_len[mb_y_len<=0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "\n",
        "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_len).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "\n",
        "            # 更新\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)     # 这里防止梯度爆炸， 这是和以往不太一样的地方\n",
        "            optimizer.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                print('Epoch', epoch, 'iteration', it, 'loss', loss.item())\n",
        "\n",
        "        print('Epoch', epoch, 'Training loss', total_loss / total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            evaluate(model, dev_data)\n",
        "\n",
        "# 训练\n",
        "train(model, train_data, num_epochs=20)"
      ],
      "metadata": {
        "id": "xkWc4nm5S5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = mask.contiguous().view(-1, 1)   # [batch_size*seq_len-1, 1]\n",
        "output = -input.gather(1, target) * mask"
      ],
      "metadata": {
        "id": "dzePWxdpS7Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention模型"
      ],
      "metadata": {
        "id": "I3EjKhXtTe_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(enc_hidden_size*2, dec_hidden_size)\n",
        "    def forward(self, x, lengths):\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[sorted_idx.long()]\n",
        "        embedded = self.dropout(self.embed(x_sorted)) #[batch_size, seq_len, embed_size]\n",
        "\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        packed_out, hid = self.rnn(packed_embedded)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)#[batch_size,seq_len,2*enc_hidden_size]\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        out = out[original_idx.long()].contiguous() #[batch_size, seq_len, 2*enc_hidden_size]\n",
        "        hid = hid[:, original_idx.long()].contiguous() #[2,batch_size, enc_hidden_size]\n",
        "\n",
        "        hid = torch.cat([hid[-2], hid[-1]], dim=1) #双向的GRU，这里是最后一个状态，联结起来 [batch_size, 2*enc_hidden_size]\n",
        "        hid = torch.tanh(self.fc(hid)).unsqueeze(0) #[1, batch_size, dec_hidden_size]\n",
        "\n",
        "        return out, hid"
      ],
      "metadata": {
        "id": "dfdsWQLsTmBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.enc_hidden_size = enc_hidden_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
        "        self.linear_out = nn.Linear(enc_hidden_size*2+dec_hidden_size, dec_hidden_size)\n",
        "\n",
        "    def forward(self, output, encoder_output, mask):\n",
        "        # output: [batch_size, seq_len_y-1, dec_hidden_size]  这个output 是decoder的每个时间步输出的隐藏状态\n",
        "        # encoder_output: [batch_size, seq_len_x, 2*enc_hidden_size]\n",
        "        batch_size = output.size(0)\n",
        "        output_len = output.size(1)\n",
        "        input_len = encoder_output.size(1)\n",
        "\n",
        "        context_in = self.linear_in(encoder_output.view(batch_size*input_len, -1))  # [batch_size*seq_len_x,dec_hidden_size]\n",
        "        context_in = context_in.view(batch_size, input_len, -1)  # [batch_size, seq_len_x, dec_hidden_size]\n",
        "        context_in = context_in.transpose(1, 2)   # [batch_size, dec_hidden_size, seq_len_x]\n",
        "\n",
        "        attn = torch.bmm(output, context_in)  # [batch_size, seq_len_y-1, seq_len_x]\n",
        "        # 这个东西就是求得当前时间步的输出output和所有输入相似性关系的一个得分score , 下面就是通过softmax把这个得分转成权重\n",
        "        attn = F.softmax(attn, dim=2)    # 此时第二维度的数字全都变成了0-1之间的数， 越大表示当前的输出output与哪个相关程度越大\n",
        "\n",
        "        context = torch.bmm(attn, encoder_output)   # [batch_size, seq_len_y-1, 2*enc_hidden_size]\n",
        "\n",
        "        output = torch.cat((context, output), dim=2)  # [batch_size, seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
        "\n",
        "        output = output.view(batch_size*output_len, -1)   # [batch_size*seq_len_y-1, 2*enc_hidden_size+dec_hidden_size]\n",
        "        output = torch.tanh(self.linear_out(output))     # [batch_size*seq_len_y-1, dec_hidden_size]\n",
        "        output = output.view(batch_size, output_len, -1)  # [batch_size, seq_len_y-1, dec_hidden_size]\n",
        "\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "tGHnPSW-Tpiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, x_len, y_len):\n",
        "        # a mask of shape x_len*y_len\n",
        "        x_mask = torch.arange(x_len.max(), device=x_len.device)[None, :] < x_len[:, None]\n",
        "        y_mask = torch.arange(y_len.max(), device=x_len.device)[None, :] < y_len[:, None]\n",
        "\n",
        "        x_mask = x_mask.float()\n",
        "        y_mask = y_mask.float()\n",
        "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, encoder_out, encoder_out_lengths, y, y_lengths, hid):\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]   # 句子从长到短排序\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "\n",
        "        y_sorted = self.dropout(self.embed(y_sorted))     # [batch_size, output_length, embed_size]\n",
        "\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()   # [batch_size, seq_len_y-1, dec_hidden_size]\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        mask = self.create_mask(y_lengths, encoder_out_lengths)\n",
        "\n",
        "        output, attn = self.attention(output_seq, encoder_out, mask)\n",
        "        output = F.log_softmax(self.out(output), -1)\n",
        "\n",
        "        return output, hid, attn"
      ],
      "metadata": {
        "id": "POMNSC0NTsSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        output, hid, attn = self.decoder(encoder_out, x_lengths, y, y_lengths, hid)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "    def translate(self, x, x_lengths, y, max_length=100):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid, attn = self.decoder(encoder_out, x_lengths, y, torch.ones(batch_size).long().to(y.device), hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "            attns.append(attn)\n",
        "\n",
        "        return torch.cat(preds, 1), torch.cat(attns, 1)"
      ],
      "metadata": {
        "id": "6J4tranpTuWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}